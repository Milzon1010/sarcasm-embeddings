{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0cf8d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7d979",
   "metadata": {},
   "source": [
    "# ---- config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9024a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "DATA_PATH = \"./data/sarcasm.json\"\n",
    "VOCAB_SIZE = 20000 \n",
    "MAX_LEN = 32\n",
    "EMBED_DIM = 16\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 6 \n",
    "\n",
    "os.makedirs(\"figs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8725857",
   "metadata": {},
   "source": [
    "# ---- load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8581b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /home/milzon/projects/sarcasm-embeddings\n",
      "Ada folder data? True\n",
      "Ada file sarcasm.json? False\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "print(\"CWD =\", os.getcwd())\n",
    "print(\"Ada folder data?\", pathlib.Path(\"data\").exists())\n",
    "print(\"Ada file sarcasm.json?\", pathlib.Path(\"data/sarcasm.json\").exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c630fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size (bytes): 5643545\n",
      "Records: 26709\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request, json, os\n",
    "\n",
    "URL = \"https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\"\n",
    "DEST = Path(\"data/sarcasm.json\")\n",
    "DEST.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# download if missing\n",
    "if not DEST.exists():\n",
    "    print(f\"Downloading to {DEST} ...\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(URL, DEST.as_posix())\n",
    "    except Exception as e:\n",
    "        # fallback simple\n",
    "        import requests\n",
    "        r = requests.get(URL, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        DEST.write_bytes(r.content)\n",
    "\n",
    "# verify\n",
    "print(\"File size (bytes):\", os.path.getsize(DEST))\n",
    "with open(DEST, \"r\") as f:\n",
    "    ds = json.load(f)\n",
    "print(\"Records:\", len(ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73d55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, \"r\") as f:\n",
    "    ds = json.load(f)\n",
    "\n",
    "texts = [d[\"headline\"] for d in ds]\n",
    "labels = np.array([d[\"is_sarcastic\"] for d in ds], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e366ba79",
   "metadata": {},
   "source": [
    "# ---- split train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c00678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 15:38:49.930521: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "N = len(texts)\n",
    "idx = np.arange(N); np.random.shuffle(idx)\n",
    "cut = int(0.9 * N)\n",
    "train_idx, val_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "train_texts = tf.data.Dataset.from_tensor_slices([texts[i] for i in train_idx])\n",
    "train_labels = tf.data.Dataset.from_tensor_slices(labels[train_idx])\n",
    "val_texts   = tf.data.Dataset.from_tensor_slices([texts[i] for i in val_idx])\n",
    "val_labels  = tf.data.Dataset.from_tensor_slices(labels[val_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafbe6e",
   "metadata": {},
   "source": [
    "# ---- vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a3b76c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 15:39:17.949529: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LEN\n",
    ")\n",
    "vectorizer.adapt(train_texts.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6b231",
   "metadata": {},
   "source": [
    "# ---- datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1d5ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.zip((train_texts.map(vectorizer), train_labels))\\\n",
    "            .shuffle(2048).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = tf.data.Dataset.zip((val_texts.map(vectorizer), val_labels))\\\n",
    "            .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c219a86",
   "metadata": {},
   "source": [
    "# ---- model (ringan aja dulu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(MAX_LEN,), dtype=\"int32\"),\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE, EMBED_DIM),              # learned embedding\n",
    "        tf.keras.layers.Conv1D(128, 5, activation=\"relu\"),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0da82",
   "metadata": {},
   "source": [
    "# ---- train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f638e612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - accuracy: 0.7177 - loss: 0.5272 - val_accuracy: 0.8476 - val_loss: 0.3589\n",
      "Epoch 2/6\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.8908 - loss: 0.2698 - val_accuracy: 0.8525 - val_loss: 0.3524\n",
      "Epoch 3/6\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.9452 - loss: 0.1500 - val_accuracy: 0.8484 - val_loss: 0.4234\n",
      "Epoch 4/6\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9750 - loss: 0.0750 - val_accuracy: 0.8401 - val_loss: 0.5243\n",
      "Epoch 5/6\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - accuracy: 0.9891 - loss: 0.0371 - val_accuracy: 0.8401 - val_loss: 0.6562\n",
      "Epoch 6/6\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9944 - loss: 0.0194 - val_accuracy: 0.8304 - val_loss: 0.7698\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b15d6",
   "metadata": {},
   "source": [
    "# ---- plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf6f6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].plot(history.history[\"accuracy\"]); ax[0].plot(history.history[\"val_accuracy\"])\n",
    "ax[0].set_title(\"Accuracy\"); ax[0].legend([\"train\",\"val\"])\n",
    "ax[1].plot(history.history[\"loss\"]); ax[1].plot(history.history[\"val_loss\"])\n",
    "ax[1].set_title(\"Loss\"); ax[1].legend([\"train\",\"val\"])\n",
    "plt.tight_layout(); plt.savefig(\"figs/training_curves.png\"); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65435f1d",
   "metadata": {},
   "source": [
    "# ---- Word embedding projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a97ce005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1de85e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figs/sentence_embedding_pca.png\n"
     ]
    }
   ],
   "source": [
    "Path(\"figs\").mkdir(exist_ok=True)\n",
    "\n",
    "# 1) Encoder = semua layer sebelum Dense(1)\n",
    "encoder = tf.keras.Sequential(model.layers[:-1])\n",
    "_ = encoder(tf.zeros((1, MAX_LEN), dtype=tf.int32))  # build sekali\n",
    "\n",
    "# 2) Kumpulkan vektor token dari val_ds (SUDAH vectorized)\n",
    "MAX_SAMPLES = 2000\n",
    "X_vec = np.stack([x.numpy() for x, _ in val_ds.unbatch().take(MAX_SAMPLES)]).astype(np.int32)\n",
    "y_vec = np.array([int(y.numpy()) for _, y in val_ds.unbatch().take(MAX_SAMPLES)])\n",
    "\n",
    "# 3) Ekstrak fitur (keluaran Dense(64)) lalu PCA\n",
    "Z = encoder.predict(X_vec, batch_size=512, verbose=0)   # shape (M, 64)\n",
    "\n",
    "coords = PCA(n_components=2, random_state=42).fit_transform(Z)\n",
    "neg = y_vec == 0; pos = ~neg\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(coords[neg,0], coords[neg,1], s=10, alpha=0.6, label=\"Not Sarcasm\")\n",
    "plt.scatter(coords[pos,0], coords[pos,1], s=10, alpha=0.6, label=\"Sarcasm\")\n",
    "plt.legend(); plt.title(\"Sentence Embeddings Projection (PCA)\")\n",
    "plt.tight_layout(); plt.savefig(\"figs/sentence_embedding_pca.png\", dpi=220); plt.close()\n",
    "print(\"Saved: figs/sentence_embedding_pca.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf77f9",
   "metadata": {},
   "source": [
    "- Export WORD embeddings → projector/word/{vectors.tsv,metadata.tsv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78c85d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "Path(\"projector/sent\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Encoder: semua layer sebelum Dense(1)\n",
    "encoder = tf.keras.Sequential(model.layers[:-1])\n",
    "_ = encoder(tf.zeros((1, MAX_LEN), dtype=tf.int32))  # build\n",
    "\n",
    "# Kumpulkan tepat sekali dari val_ds (yang SUDAH vectorized)\n",
    "MAX_SAMPLES = 2000\n",
    "X_list, y_list = [], []\n",
    "for x, y in val_ds.unbatch().take(MAX_SAMPLES):\n",
    "    X_list.append(x.numpy())\n",
    "    y_list.append(int(y.numpy()))\n",
    "\n",
    "X = np.stack(X_list).astype(np.int32)\n",
    "y = np.array(y_list, dtype=np.int32)\n",
    "\n",
    "# Hitung embedding kalimat\n",
    "Z = encoder.predict(X, batch_size=512, verbose=0)  # shape: (M, D)\n",
    "M = Z.shape[0]                                     # jumlah baris yang akan disimpan\n",
    "\n",
    "# Pastikan metadata dipotong ke M baris\n",
    "y = y[:M]\n",
    "\n",
    "# Tulis vectors.tsv dan metadata.tsv (dengan newline terakhir)\n",
    "np.savetxt(\"projector/sent/vectors.tsv\", Z, delimiter=\"\\t\")\n",
    "with open(\"projector/sent/metadata.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(M):\n",
    "        f.write(f\"{y[i]}\\n\")   # pastikan newline di akhir setiap baris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbec00a",
   "metadata": {},
   "source": [
    "- Export SENTENCE embeddings → projector/sent/{vectors.tsv,metadata.tsv}\n",
    "- Kita ambil keluaran Dense(64) (layer sebelum output).\n",
    "- Gunakan dataset yang sudah di-vectorize (int32). Jangan panggil vectorizer() lagi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de60b8",
   "metadata": {},
   "source": [
    "# word projection\n",
    "- supaya bisa search kata kata di projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb53385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors: 5000 metadata(data rows): 5000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np, tensorflow as tf\n",
    "\n",
    "Path(\"projector/word\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ambil embedding & vocab\n",
    "embedding_layer = next(l for l in model.layers if isinstance(l, tf.keras.layers.Embedding))\n",
    "E = embedding_layer.get_weights()[0]\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab_eff = min(len(vocab), E.shape[0])\n",
    "\n",
    "START = 2                                   # skip '' dan [UNK]\n",
    "N = min(5000, max(0, vocab_eff - START))\n",
    "idx = np.arange(START, START+N)\n",
    "\n",
    "# Simpan vektor\n",
    "np.savetxt(\"projector/word/vectors.tsv\", E[idx], delimiter=\"\\t\")\n",
    "\n",
    "# Simpan metadata **2 kolom + header** -> baris = N + 1\n",
    "with open(\"projector/word/metadata.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"token\\trank\\n\")                # header WAJIB utk multi-kolom\n",
    "    for r, i in enumerate(idx):\n",
    "        tok = vocab[i].replace(\"\\t\",\" \").replace(\"\\n\",\" \")\n",
    "        f.write(f\"{tok}\\t{r}\\n\")\n",
    "\n",
    "# Verifikasi jumlah baris\n",
    "vec_rows = sum(1 for _ in open(\"projector/word/vectors.tsv\", \"r\", encoding=\"utf-8\"))\n",
    "meta_rows = sum(1 for _ in open(\"projector/word/metadata.tsv\", \"r\", encoding=\"utf-8\")) - 1  # minus header\n",
    "print(\"vectors:\", vec_rows, \"metadata(data rows):\", meta_rows)  # harus sama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6056c66e",
   "metadata": {},
   "source": [
    "- Hapus semua projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2db2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf projector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe42221",
   "metadata": {},
   "source": [
    "- menyimpan teks validasi mentah (mis. val_texts dari tahap split awal), kbisa bikin metadata 2 kolom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cbfebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"projector/sent/metadata.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for lbl, txt in zip(y_vec, val_texts_list[:len(y_vec)]):\n",
    "        txt = str(txt).replace(\"\\t\",\" \").replace(\"\\n\",\" \")\n",
    "        f.write(f\"{lbl}\\t{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74348f7b",
   "metadata": {},
   "source": [
    "- eval & sample predictions\n",
    "- ambil seluruh val untuk Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc29e9",
   "metadata": {},
   "source": [
    "# Cara menayangkan di web\n",
    "* Buka https://projector.tensorflow.org/\n",
    "\n",
    "- Klik Load → unggah berkas vectors.tsv dan metadata.tsv.\n",
    "- Di panel kanan, pilih PCA atau UMAP untuk eksplorasi.\n",
    "- Color by: label (untuk proyeksi kalimat/sentence).\n",
    "- Label by: token (untuk proyeksi kata/word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01a73c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.855     0.839     0.847      1492\n",
      "           1      0.801     0.819     0.810      1179\n",
      "\n",
      "    accuracy                          0.830      2671\n",
      "   macro avg      0.828     0.829     0.828      2671\n",
      "weighted avg      0.831     0.830     0.831      2671\n",
      "\n",
      "[[1252  240]\n",
      " [ 213  966]]\n"
     ]
    }
   ],
   "source": [
    "val_X = np.asarray(list(val_texts.map(vectorizer).as_numpy_iterator()))\n",
    "val_y = labels[val_idx]\n",
    "pred  = (model.predict(val_X, verbose=0).ravel() > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(val_y, pred, digits=3))\n",
    "print(confusion_matrix(val_y, pred))\n",
    "# ---- eval & sample predictions\n",
    "# ambil seluruh val untuk Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1dc173",
   "metadata": {},
   "source": [
    "# simpan contoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cec10fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, another delay… -> Sarcasm p=0.93\n",
      "Thank you so much 🙏 -> Sarcasm p=0.21\n",
      "Yeah right, that meeting was super useful -> Sarcasm p=0.09\n",
      "I absolutely love waiting in traffic for hours -> Sarcasm p=0.12\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    \"Great, another delay…\",\n",
    "    \"Thank you so much 🙏\",\n",
    "    \"Yeah right, that meeting was super useful\",\n",
    "    \"I absolutely love waiting in traffic for hours\"\n",
    "]\n",
    "sample_vec = vectorizer(tf.constant(samples))\n",
    "probs = model.predict(sample_vec, verbose=0).ravel()\n",
    "for s, p in zip(samples, probs):\n",
    "    print(f\"{s} -> Sarcasm p={p:.2f}\")\n",
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4358bfd3",
   "metadata": {},
   "source": [
    "# simpan artefak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a820f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"artifacts\").mkdir(exist_ok=True)\n",
    "Path(\"figs\").mkdir(exist_ok=True)\n",
    "\n",
    "model.save(\"artifacts/sarcasm_embed_model.keras\")\n",
    "\n",
    "with open(\"figs/sample_preds.txt\",\"w\") as f:\n",
    "    for s, p in zip(samples, probs):\n",
    "        f.write(f\"{s}\\tSarcasm p={p:.2f}\\n\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
